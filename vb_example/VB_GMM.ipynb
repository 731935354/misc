{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Modeling using Variational Bayes\n",
    "This example shows a detailed derivation of Gaussian mixture model (GMM) parameters using Variational Bayes (VB). We will see that, similar to EM for Maximum Likelihood, VB also results in iterative updates of statistics (i.e., responsibilities -- E-step) and model paramaters (i.e., M-step). This derivation is based entirely on Chapter 10 of PRML - Bishop 2006. \n",
    "\n",
    "\n",
    "## Latent variables\n",
    "\n",
    "Just like any VB approach, we must first identify the latent variable space. For a GMM, four latent variables are used. For $K$ mixtures and $N$ samples, define:\n",
    "\n",
    "- ${\\bf Z} = [{z}_{n,k}]$: the random variable that determines whether the $n^{th}$ sample was generated from the $k^{th}$ Gaussian mixture. By definition $z_{n,k}$ are either $0$ or $1$. \n",
    "- ${\\bf \\pi} = [{\\pi}_k]$: probability of the $k^{th}$ mixture. \n",
    "- ${\\bf \\mu} = [{\\bf \\mu}_k]$: $p\\times1$ mean of the $k^{th}$ mixture. \n",
    "- ${\\bf \\Lambda} = [{\\bf \\Lambda}_k]$: $p\\times p$ precision (inverse covariance) of the $k^{th}$ mixture. \n",
    "\n",
    "where $k = 1\\dots K$ and $n = 1\\dots N$. All for latent variables ${\\bf Z}, {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Lambda}$ contribute to the set of $p$-dimensional observations ${\\bf X} = [{\\bf x}_n]$, for $n = 1\\dots N$.\n",
    "\n",
    "NOTE: The nessecity of all the latent variables, especially ${\\bf Z}$, should be clear to the readers. \n",
    "\n",
    "NOTE: Using our notation in the [introduction to VB](link), ${\\bf Z}_1, \\dots, {\\bf Z}_4$ correspond to ${\\bf Z}$, ${\\bf \\pi}$, ${\\bf \\mu}$, and ${\\bf \\Lambda}$. \n",
    "\n",
    "## Derivation:\n",
    "Our derivations follow a step-by-step approach in order to provide an outline for the steps required to apply VB to any other problem. At this point, we have established the latent variables and their relation to the observations. We may also benefit from the probabilistic graphical model (PGM) shown in Fig. 1, which shows the Bayesian relations underlying the latent variables and observations. \n",
    "\n",
    "\n",
    "### Step 1: Determine the Likelihood function\n",
    "Using Fig. 1 as a guideline and the definitions in the previous section, we may explicitly write the likelihood of ${\\bf X}$ given ${\\bf Z},{\\bf \\mu},{\\bf \\Lambda}$, and similarly ${\\bf Z}$ given ${\\bf \\pi}$: \n",
    "$$p({\\bf X}\\mid{\\bf Z},{\\bf \\pi},{\\bf \\mu},{\\bf \\Lambda}) = \\prod_{n=1}^{N}\\prod_{k=1}^{K}\\mathcal{N}({\\bf x}_n\\mid{\\bf \\mu}_k,{\\bf \\Lambda}_k^{-1})^{z_{n,k}}$$\n",
    "and \n",
    "$$p({\\bf Z}\\mid{\\bf \\pi}) = \\prod_{n=1}^{N}\\prod_{k=1}^{K} \\pi_k^{z_{n,k}}$$\n",
    "\n",
    "\n",
    "### Step 2: Joint distribution of all variables\n",
    "In the next step, we write down the joint distribution of all variables, also known as the probability of the *complete dataset*. The joint distribution must be expressed in terms of the likelihood functions and latent variable priors. \n",
    "\n",
    "$$p({\\bf X},{\\bf Z}, {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Lambda}) = p({\\bf X}\\mid{\\bf Z}, {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Lambda})p({\\bf Z}\\mid{\\bf \\pi})p({\\bf \\pi})p({\\bf \\mu}\\mid{\\bf \\Lambda})p({\\bf \\Lambda})$$\n",
    "\n",
    "The expansion above was written in accordance with the PGM in Fig. 1. \n",
    "\n",
    "### Step 3: Determine the Variational distribution \n",
    "The variational distribution $q(.)$ is the distribution of the latent space. Recall the end goal of VB provides an optimal solution for $q$. For the case of GMM, we use the mean-field approximation (MFA) as described in [introduction to VB](link). The MFA for GMM only separates ${\\bf Z}$ from the GMM parameters.  \n",
    "\n",
    "$$q({\\bf Z}, {\\bf \\pi}, {\\bf \\mu}, {\\bf \\Lambda}) = q({\\bf Z})q({\\bf \\pi}, {\\bf \\mu}, {\\bf \\Lambda})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
