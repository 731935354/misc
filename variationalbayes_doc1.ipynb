{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinate Ascent Update for Mean-Field Variational Bayes\n",
    "Variational Bayes (Variational Inference) is a functional optimization procedure designed to find the best approximating distribution, $q({\\bf Z})$, that results in minimum Kullback-Leibler divergence between itself and the posterior distribution of the latent variables given some data, $p({\\bf Z}\\big\\rvert {\\bf X})$. ${\\bf Z}$ and ${\\bf X}$ respectively represent the collection of latent and observation variables.\n",
    "\n",
    "### Problem Statement\n",
    "We follow the interpretation presented in [1]. Let ${\\bf X}$ be a set of observations (random variables), ${\\bf Z}$ be some unknown random latent variables. From Bayes's theorem: \n",
    "$$p({\\bf X}) = \\frac{p({\\bf X,\\bf Z})}{p({\\bf Z\\big\\rvert \\bf X})}\\\\\n",
    "$$\n",
    "$$\\hspace{4cm} \\log(p({\\bf X})) = \\log{p({\\bf X, \\bf Z})} - \\log{p({\\bf Z\\big\\rvert \\bf X})}\\\\\n",
    "$$\n",
    "Taking the expectation with respect to some distribution, $q({\\bf Z})$, gives:\n",
    "\n",
    "$$\n",
    "\\log(p({\\bf X})) = E_q[\\log{p({\\bf X,\\bf Z})}] - E_q[\\log{p({\\bf Z\\big\\rvert \\bf X})}],\n",
    "$$\n",
    "\n",
    "where $E_q[f({\\bf X,\\bf Z})] = \\int q({\\bf Z})f({\\bf X ,\\bf Z})d{\\bf Z}$ and we have used $\\log(p({\\bf X})) = E_q[\\log(p({\\bf X}))]$, since $p({\\bf X})$ does not depend on ${\\bf Z}$. After adding $E_q[\\log(q({\\bf Z}))]$ to and subtracting it from the r.h.s: \n",
    "\n",
    "$$\n",
    "\\log(p({\\bf X})) = E_q[\\log{\\frac{p({\\bf X, \\bf Z})}{q({\\bf Z})}}] - E_q[\\log{\\frac{p({\\bf Z\\big\\rvert \\bf X})}{q({\\bf Z})}}].\n",
    "$$\n",
    "\n",
    "We know that the marginal distribution of the observations, $p({\\bf X})$, is constant. Therefore, minimizing $E_q[\\log{\\frac{p({\\bf Z\\big\\rvert \\bf X})}{q({\\bf Z})}}]$, which is the KL divergence between $q({\\bf Z})$ and the posterior distribution, is equivalent to maximizing $\\mathcal{L}(q) = E_q[\\log{\\frac{p({\\bf X,\\bf Z})}{q({\\bf Z})}}]$. This maximization forms the **objective of Variational Bayes**. \n",
    "\n",
    "The next section describes a special case of VB maximization called Mean-Field approximation (MFA), where $q({\\bf Z}) = \\prod_{i=1}^{K}q_i({\\bf Z}_i)$. The integer $K$ is the number of latent variables. \n",
    "\n",
    "\n",
    "### Maximizing the VB objective\n",
    "We start by inserting the mean-field approximation into the components of $\\mathcal{L}(q) =  E_q[\\log{{p({\\bf X,Z})}]-E_q[\\log{q({\\bf Z})}}]$, separately. The following approach has been adopted from [2]. \n",
    "\n",
    "**Part 1:**    $E_q[\\log q({\\bf Z})]$\n",
    "\n",
    "$$\n",
    "E_q[\\log{q({\\bf Z})}] = {\\bf\\displaystyle\\int}{q({\\bf Z})\\log{q({\\bf Z})}}d{\\bf Z}\n",
    "$$\n",
    " where ${\\bf\\displaystyle\\int}f({\\bf Z})d{\\bf Z}$ represents $K$ integrations for each ${\\bf Z}_i, i = 1\\dots K$.\n",
    "Introduce MFA into the expression of $q({\\bf Z})$:\n",
    "$$\n",
    "E_q[\\log{q({\\bf Z})}] =  {\\bf\\displaystyle\\int}{\\prod_{j}q_j({\\bf Z}_j)\\log{\\prod_{i}q_i({\\bf Z}_i)}}d{\\bf Z}_1\\dots d{\\bf Z}_K\\\\\n",
    "$$\n",
    "$$\\hspace{2cm} =  {\\bf\\displaystyle\\int}{\\prod_{j}q_j({\\bf Z}_j)\\sum_{i}\\log{q_i({\\bf Z}_i)}}d{\\bf Z}_1\\dots d{\\bf Z}_K\\\\\n",
    "$$\n",
    "$$\\hspace{2cm} =  \\sum_{i}{\\bf\\displaystyle\\int}{\\prod_{j}q_j({\\bf Z}_j)\\log{q_i({\\bf Z}_i)}}d{\\bf Z}_1\\dots d{\\bf Z}_K\\\\\n",
    "$$\n",
    "$$\\hspace{2.5cm} =  \\sum_{i}\\displaystyle\\int\\dots\\displaystyle\\int{\\prod_{j}q_j({\\bf Z}_j)\\log{q_i({\\bf Z}_i)}}d{\\bf Z}_1\\dots d{\\bf Z}_K\\\\\n",
    "$$\n",
    "In the last equation, ${\\bf\\displaystyle\\int}$ has been broken down into $\\displaystyle\\int\\dots\\displaystyle\\int$. \n",
    " It is straight-forward to show that for each $i$ in the summation: $$\\displaystyle\\int\\dots\\displaystyle\\int{\\prod_{j}q_j({\\bf Z}_j)\\log{q_i({\\bf Z}_i)}}d{\\bf Z}_1\\dots d{\\bf Z}_K = \\int{q_i({\\bf Z}_i)\\log{q_i({\\bf Z}_i)}}d{\\bf Z}_i$$\n",
    "\n",
    ", which uses the property of the distribution $\\displaystyle\\int q_j({\\bf Z}_j)d{\\bf Z}_j = 1$. Therefore,\n",
    "$$E_q[\\log{q({\\bf Z})}]  =  \\sum_{i}E_{q_i}[\\log{q_i({\\bf Z}_i)}].\n",
    "$$\n",
    "The term $\\sum_iE_{q_i}[\\log(q_i({\\bf Z}_i))]$ can further be decomposed as:\n",
    "$$\n",
    "\\sum_iE_{q_i}[\\log(q_i({\\bf Z}_i))] = E_{q_j}[\\log(q_j({\\bf Z}_j))] + \\sum_{i\\ne j}E_{q_i}[\\log(q_i({\\bf Z}_i))] \\hspace{1cm}(1)$$.\n",
    "\n",
    "**Part 2:**    $E_q[\\log p({\\bf X,\\bf Z})]$\n",
    "\n",
    "The other reformulation is obtained by applying the chain rule of probabilities to $p({\\bf X,\\bf Z})$.\n",
    "for any $1\\le j \\le K$\n",
    "\n",
    "$$p({\\bf X},{\\bf Z}_1 \\dots {\\bf Z}_K) = p({\\bf X}) p({\\bf Z}_1\\dots{\\bf Z}_{j-1},{\\bf Z}_{j+1},\\dots,{\\bf Z}_{K}\\big\\rvert{\\bf X})p({\\bf Z}_j\\big\\rvert {\\bf X},{\\bf Z}_1\\dots{\\bf Z}_{j-1},{\\bf Z}_{j+1},\\dots,{\\bf Z}_{K})$$\n",
    "\n",
    "We can define $\\bar{\\bf Z}_{j}$ as short for the collection of all latent variables, except ${\\bf Z}_j$. \n",
    "\n",
    "$$p({\\bf X},{\\bf Z}_1 \\dots {\\bf Z}_K) = p({\\bf X}) p(\\bar{\\bf Z}_j\\big\\rvert{\\bf X})p({\\bf Z}_j\\big\\rvert {\\bf X},\\bar{\\bf Z}_j)$$\n",
    "\n",
    "Therefore, \n",
    "\n",
    "$$\\log p({\\bf X},{\\bf Z}) = \\log p({\\bf X}) + \\log p({\\bf Z}_j\\big\\rvert {\\bf X},\\bar{\\bf Z}_j) + \\log p(\\bar{\\bf Z}_j\\big\\rvert{\\bf X})$$\n",
    "\n",
    "Inserting the resulting expression for $\\log p({\\bf X},{\\bf Z})$ into $E_q[\\log p({\\bf X,\\bf Z})]$: \n",
    "\n",
    "$$\n",
    "E_q[\\log p({\\bf X,\\bf Z})] = \\displaystyle\\int\\dots\\displaystyle\\int\\prod_{i}q_i({\\bf Z}_i)\\big(\\log p({\\bf X}) + \\log p({\\bf Z}_j\\big\\rvert {\\bf X},\\bar{\\bf Z}_j) + \\log p(\\bar{\\bf Z}_j\\big\\rvert{\\bf X})\\big)d{\\bf Z}_1\\dots d{\\bf Z}_K\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hspace{2cm} = \\log p({\\bf X}) + {\\bf\\displaystyle\\int}\\prod_{i\\ne j}q_i({\\bf Z}_i)\\log p(\\bar{\\bf Z}_j\\big\\rvert{\\bf X})d\\bar{\\bf Z}_j + \n",
    "\\displaystyle\\int q_j({\\bf Z}_j)\\Big({\\bf\\displaystyle\\int}\\prod_{i\\ne j}q_i({\\bf Z}_i)\\log p({\\bf Z}_j\\big\\rvert{\\bf X},\\bar{\\bf Z}_j)d\\bar{\\bf Z}_j\\Big) d{\\bf Z}_j\n",
    "$$\n",
    "\n",
    "Similarly, $\\bar{q}_j$ is the collection of all p.d.fs, except $q_j$. \n",
    "\n",
    "$$\n",
    "E_q[\\log p({\\bf X,\\bf Z})] = \\log p({\\bf X}) + E_{\\bar{q}_j}[\\log p(\\bar{\\bf Z}_j\\big\\rvert{\\bf X})] + E_{q_j}[E_{\\bar{q}_j}[\\log p({\\bf Z}_j\\big\\rvert\\bar{\\bf Z}_j,{\\bf X})]]\\hspace{1cm}(2)\n",
    "$$\n",
    "\n",
    "Notice that the first two terms do not depend on $q({\\bf Z}_j)$. \n",
    "\n",
    "Using (1) and (2), we can now rewrite $\\mathcal{L}(q)$: \n",
    "\n",
    "$$\n",
    "\\mathcal{L}(q) = \\log p({\\bf X}) + E_{\\bar{q}_j}[\\log p(\\bar{\\bf Z}_j\\big\\rvert{\\bf X})] + E_{q_j}[E_{\\bar{q}_j}[\\log p({\\bf Z}_j\\big\\rvert\\bar{\\bf Z}_j,{\\bf X})]] - E_{q_j}[\\log(q_j({\\bf Z}_j))] - \\sum_{i\\ne j}E_{q_i}[\\log(q_i({\\bf Z}_i))]\n",
    "\\hspace{1cm}(3)$$ \n",
    "\n",
    "Note that the choice of $j$ is completely arbitrary. \n",
    "\n",
    "### Coordinate Ascent\n",
    "Separating each individual $q_j$ in the expression for $\\mathcal{L}(q)$ allows a step-by-step maximization along each $q_j$. To find the maximum point for $q_j$, we must calculate the functional derivative of $\\mathcal{L}(q)$ with respect to $q_j$. For this derivative, we can ignore the terms that do not depend on $q_j$, leaving us with the two middle terms in (3).\n",
    "$$\\frac{\\partial\\mathcal{L}(q)}{\\partial q_j} = \\frac{\\partial}{\\partial q_j}.$$ \n",
    "$$\n",
    "                                   E_{\\bar{q}_j}\\big[p({\\bf Z}_j\\big\\rvert {\\bf X},{\\bf Z}_{\\bar{j}})\\big] - \\log(q_j({\\bf Z}_j)) - 1 = 0\n",
    "$$\n",
    "where we have used the well-known results functional derivatives for entropy (i.e., $\\frac{\\partial }{\\partial p}\\int p\\log(p)dx = \\log(p)+1$). The stationary point from above results in: \n",
    "\n",
    "$$\n",
    "\\log(q_j({\\bf Z}_j)) + \\log(e) = E_{\\bar{q}_j}\\big[p({\\bf Z}_j\\big\\rvert {\\bf X},{\\bf Z}_{\\bar{j}})\\big]\\\\\n",
    "\\log(q_j({\\bf Z}_j)e) = E_{\\bar{q}_j}\\big[p({\\bf Z}_j\\big\\rvert {\\bf X},{\\bf Z}_{\\bar{j}})\\big]\\\\\n",
    "q_j({\\bf Z}_j) \\propto \\exp[E_{\\bar{q}_j}\\big[p({\\bf Z}_j\\big\\rvert {\\bf X},{\\bf Z}_{\\bar{j}})\\big]]\n",
    "$$\n",
    "The r.h.s does not depend on $q_j$. \n",
    "\n",
    "### References\n",
    "[1] Bishop, C., 2007. Pattern Recognition and Machine Learning (Information Science and Statistics), 1st edn. 2006. corr. 2nd printing edn. Springer, New York.\n",
    "\n",
    "[2] Neiswanger, W., 2017. Probabilistic Graphical Models, Sprint 2017 lectures, Carnegie Mellon University. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
